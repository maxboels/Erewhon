# ONNX & Quantization Formats Explained

**Your Questions Answered:**
1. What does "model parsing ONNX" mean?
2. Are there different quantization formats for Pi 5 with/without Hailo AI HAT?

---

## ğŸ“š Q1: What is ONNX and Model Parsing?

### ONNX (Open Neural Network Exchange)

**Think of ONNX as a "universal translator" for neural networks.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PyTorch    â”‚ â”€â”
â”‚   Model     â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚          â”‚    â”‚  - ONNX Runtime â”‚
â”‚ TensorFlow  â”‚ â”€â”¼â”€â”€â†’ â”‚   ONNX   â”‚ â”€â”€â†’â”‚  - TensorRT     â”‚
â”‚   Model     â”‚  â”‚    â”‚  Format  â”‚    â”‚  - Hailo NPU    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚          â”‚    â”‚  - CoreML       â”‚
                 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  - OpenVINO     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   Other     â”‚ â”€â”˜
â”‚ Frameworks  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What "Parsing" Means

**Parsing ONNX** = Converting your model from one format to ONNX's standardized format

**Example: PyTorch â†’ ONNX**

```python
# PyTorch operations (framework-specific)
class MyModel(nn.Module):
    def forward(self, x):
        x = self.linear(x)      # PyTorch Linear
        x = F.relu(x)           # PyTorch ReLU
        return x

# â†“ ONNX Export "parses" this into...

# ONNX operators (standardized)
graph {
  node {
    op_type: "MatMul"         # Linear â†’ MatMul + Add
    op_type: "Add"
  }
  node {
    op_type: "Relu"           # ReLU â†’ Relu
  }
}
```

**Why this matters:**
- âœ… Your model can run on **any hardware** that supports ONNX
- âœ… Framework-agnostic (PyTorch, TensorFlow, etc.)
- âœ… Optimized for specific hardware (Hailo, TensorRT, etc.)

### When You Need ONNX

**You DON'T need ONNX if:**
- Running on CPU with PyTorch âœ… (your current setup)
- Using standard PyTorch inference

**You DO need ONNX for:**
- Hailo AI HAT deployment ğŸš€
- Cross-platform deployment
- Hardware accelerators (NPU, TPU)
- Optimized inference engines

---

## ğŸ”„ Q2: Different Quantization Formats

**YES! The quantization format is COMPLETELY DIFFERENT with vs without the Hailo HAT.**

### Scenario 1: Pi 5 WITHOUT Hailo HAT (Current)

**Format:** PyTorch `.pth` (INT8 quantized)

```python
# Quantization method
torch.backends.quantized.engine = 'qnnpack'  # ARM CPU backend
model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)

# Save
torch.save(model.state_dict(), 'model_quantized.pth')
```

**Properties:**
- File: `.pth` (PyTorch checkpoint)
- Backend: QNNPACK (ARM CPU optimized)
- Runtime: PyTorch inference on CPU
- Latency: ~40ms
- Power: 11W

**Deployment:**
```bash
./deploy_quantized_model.sh best_model.pth episodes
# Output: model_quantized.pth (14 MB)
```

---

### Scenario 2: Pi 5 WITH Hailo AI HAT (Future)

**Format:** Hailo `.hef` (Hailo Executable Format)

```bash
# Complete different pipeline!
PyTorch Model (.pth)
    â†“
ONNX Model (.onnx)
    â†“
Hailo Parser â†’ Hailo Optimized (.har)
    â†“
Hailo Compiler â†’ Hailo Executable (.hef)
```

**Properties:**
- File: `.hef` (Hailo-specific binary)
- Backend: Hailo NPU (13 TOPS)
- Runtime: Hailo Runtime (not PyTorch!)
- Latency: ~10-15ms
- Power: 8-9W

**Deployment:**
```bash
# Step 1: Export to ONNX
python export_act_to_onnx.py --checkpoint best_model.pth --output act.onnx

# Step 2: Compile for Hailo (on x86_64 dev machine)
hailo parser onnx act.onnx --output act.har
hailo optimize act.har --hw-arch hailo8l --output act_opt.har
hailo compiler act_opt.har --output act.hef

# Step 3: Deploy HEF to Pi
scp act.hef pi@raspberrypi:~/
python3 hailo_inference.py --hef act.hef
```

---

## ğŸ“Š Comparison Table

| Aspect | **CPU (No HAT)** | **Hailo NPU (With HAT)** |
|--------|------------------|--------------------------|
| **Model Format** | `.pth` (PyTorch) | `.hef` (Hailo) |
| **Intermediate** | Not needed | `.onnx` required |
| **Quantization Type** | PyTorch INT8 (symmetric) | Hailo INT8 (asymmetric, optimized) |
| **Quantization Tool** | PyTorch quantization API | Hailo Dataflow Compiler |
| **Backend** | QNNPACK (ARM CPU) | Hailo NPU (13 TOPS) |
| **Runtime** | PyTorch | Hailo Runtime |
| **Compilation** | Not needed | Required (on x86_64) |
| **Latency** | ~40ms | ~10-15ms âš¡ |
| **Power** | 11W | 8-9W |
| **Setup Complexity** | Easy âœ… | Medium ğŸ”§ |
| **Use Now?** | **YES** âœ… | Wait for HAT ğŸ“¦ |

---

## ğŸ› ï¸ Key Differences Explained

### 1. Quantization Method

**CPU (QNNPACK):**
```python
# Symmetric INT8 quantization
# Range: -128 to 127 (centered at 0)
quantized_value = round(float_value / scale)
```

**Hailo NPU:**
```python
# Asymmetric INT8 quantization
# Range: 0 to 255 or custom range
quantized_value = round((float_value - zero_point) / scale)
```

Hailo's method better preserves accuracy for activations with non-zero mean.

### 2. Compilation Pipeline

**CPU:** No compilation needed
```
PyTorch Model â†’ Quantize â†’ Save .pth â†’ Run on Pi âœ…
```

**Hailo:** Multi-step compilation
```
PyTorch â†’ ONNX â†’ Parse â†’ Optimize â†’ Compile â†’ .hef â†’ Run on Pi NPU ğŸš€
```

### 3. Runtime Environment

**CPU:**
```python
import torch
model = torch.load('model.pth')
output = model(input)  # PyTorch inference
```

**Hailo:**
```python
from hailo_platform import VDevice
device = VDevice()
network = device.configure('model.hef')[0]
output = network.infer(input)  # Hailo NPU inference
```

Completely different APIs!

---

## ğŸ¯ What This Means for You

### **RIGHT NOW (No HAT):**

1. âœ… **Use PyTorch INT8 quantization I implemented**
   ```bash
   ./deploy_quantized_model.sh best_model.pth episodes
   ```

2. âœ… **Deploy to Pi with CPU inference**
   ```bash
   python3 lerobot_act_inference_rpi5.py --checkpoint model.pth
   ```

3. âœ… **You'll get ~40ms latency, 25-30 Hz control**
   - Totally usable!
   - No special hardware needed
   - Works immediately

**You do NOT need ONNX for this.** PyTorch â†’ Quantized PyTorch â†’ Done.

---

### **WHEN HAT ARRIVES:**

1. ğŸ”„ **Export to ONNX** (I created the script for you)
   ```bash
   python export_act_to_onnx.py --checkpoint best_model.pth --output act.onnx
   ```

2. ğŸ”„ **Compile for Hailo** (need x86_64 machine)
   ```bash
   # Install Hailo Dataflow Compiler
   # Then:
   hailo parser onnx act.onnx
   hailo optimize --hw-arch hailo8l --calib-dataset calibration/
   hailo compiler --output act.hef
   ```

3. ğŸš€ **Deploy HEF to Pi**
   ```bash
   scp act.hef pi@raspberrypi:~/
   python3 hailo_inference.py --hef act.hef
   ```

4. ğŸš€ **Enjoy 10-15ms latency, 100 Hz control!**

---

## ğŸ¤” Common Questions

### Q: "Can I use the same .pth file for both CPU and Hailo?"

**A:** No! They need different formats:
- CPU: `.pth` (PyTorch quantized)
- Hailo: `.hef` (Hailo compiled binary)

But you can convert: `.pth` â†’ ONNX â†’ `.hef`

### Q: "Do I need to retrain for Hailo?"

**A:** No! Use the same trained model:
1. Train once: `best_model.pth`
2. For CPU: Quantize with PyTorch
3. For Hailo: Export â†’ ONNX â†’ Compile to HEF

Same model, different deployment formats.

### Q: "Why not use ONNX Runtime on CPU?"

**A:** You could, but PyTorch is simpler for CPU-only:
- PyTorch: Native, works out of box âœ…
- ONNX Runtime: Extra dependency, same performance

ONNX shines for **hardware accelerators** (Hailo, TensorRT).

### Q: "Should I wait for HAT before deploying?"

**A:** NO! Deploy on CPU now:
- Works immediately
- Good enough for testing
- Easy upgrade path when HAT arrives

---

## ğŸ“‹ Summary

### ONNX Parsing:
- **What it is:** Converting model operations to standardized ONNX format
- **When you need it:** For Hailo HAT, cross-platform deployment
- **When you don't:** CPU-only PyTorch inference (your current case)

### Quantization Formats:

**Without HAT:**
- Format: PyTorch `.pth` (INT8)
- Use: `deploy_quantized_model.sh` âœ…
- Result: ~40ms, works now

**With HAT:**
- Format: Hailo `.hef` (custom INT8)
- Use: ONNX â†’ Hailo compiler ğŸš€
- Result: ~10-15ms, when HAT arrives

### Recommendation:

1. **NOW:** Use CPU quantization (already implemented)
2. **Test:** Validate 30Hz control works
3. **LATER:** When HAT arrives, I'll help you compile for Hailo
4. **Upgrade:** ~40ms â†’ ~10ms, easy transition!

---

## ğŸ“ Files Ready for You

### Available NOW (CPU deployment):
- âœ… `quantize_act_model.py` - PyTorch INT8 quantization
- âœ… `lerobot_act_inference_rpi5.py` - CPU inference
- âœ… `deploy_quantized_model.sh` - Automated pipeline

### Ready for Hailo HAT:
- âœ… `export_act_to_onnx.py` - ONNX export (just created)
- ğŸ“¦ `hailo_compile.sh` - Compilation script (I can create when needed)
- ğŸ“¦ `hailo_inference.py` - NPU inference (I can create when needed)

---

**Bottom Line:**

- **ONNX = Universal model format** (like PDF for neural networks)
- **Different hardware = Different quantization formats**
- **CPU (now) = PyTorch .pth** âœ…
- **Hailo (later) = Hailo .hef** ğŸš€
- **You're ready to deploy on CPU immediately!**

Questions? Let me know! ğŸš€
