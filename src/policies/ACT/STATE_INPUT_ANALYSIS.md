# üîç ACT Model State Input Analysis

## ‚ùì Question: Do We Pass Current States (Steering & Throttle) to the ACT Model?

## ‚úÖ **Answer: YES, but with Important Differences Between Implementations**

---

## üìä Summary by Implementation

### 1. **Official ACT Trainer** (`official_act_trainer.py`) ‚úÖ **USES STATE**

**Configuration:**
```python
input_features={
    'observation.images.front': PolicyFeature(
        type=FeatureType.VISUAL,
        shape=(3, 480, 640)
    ),
    'observation.state': PolicyFeature(
        type=FeatureType.STATE,
        shape=(2,)  # [steering, throttle]
    ),
}
```

**Data Format:**
```python
formatted_item = {
    'observation.images.front': image_tensor,     # Camera image [3, 480, 640]
    'observation.state': state_tensor,            # Current state [steering, throttle]
    'action': action_tensor,                      # Target action [steering, throttle]
}
```

**‚úÖ The official ACT policy receives:**
- Camera image (640x480)
- **Current state: [steering, throttle]** (2D vector)
- Predicts: Next action [steering, throttle]

---

### 2. **Train Local ACT** (`train_local_act.py`) ‚úÖ **USES STATE**

**Configuration:**
```python
input_features = {
    'observation.image_front': PolicyFeature(
        type=FeatureType.VISUAL,
        shape=(3, 480, 640)
    ),
    'observation.state': PolicyFeature(
        type=FeatureType.STATE,
        shape=(2,)  # [steering, throttle]
    )
}
```

**‚úÖ This implementation also uses state as input**

---

### 3. **Simple ACT Trainer** (`simple_act_trainer.py`) ‚ùå **DOES NOT USE STATE**

**Model Architecture:**
```python
def forward(self, images):
    batch_size = images.shape[0]
    
    # Encode images
    visual_features = self.vision_encoder(images)  # Only uses images!
    
    # Add sequence dimension for transformer
    visual_features = visual_features.unsqueeze(1)
```

**Dataset Output:**
```python
# Action vector [steering, throttle]
action = torch.tensor([sample['steering'], sample['throttle']], dtype=torch.float32)

return image, action  # Only returns image and action, NO state
```

**‚ùå This simplified version is image-only:**
- Input: Camera image (640x480)
- Output: Action [steering, throttle]
- **Does NOT use current state as input**

---

### 4. **Enhanced ACT Trainer** (`enhanced_act_trainer.py`) ‚ùå **DOES NOT USE STATE**

```python
# Imports from simple_act_trainer
from simple_act_trainer import SimplifiedTracerDataset, SimpleACTModel
```

**‚ùå Uses the same image-only model as `simple_act_trainer.py`**

---

## üîÑ Data Flow Comparison

### **Official ACT (State-Aware)**
```
Input Pipeline:
1. Camera Frame (640√ó480) ‚Üí Image Tensor [3, 480, 640]
2. Current State [steering, throttle] ‚Üí State Tensor [2]
3. Concatenated Features ‚Üí Transformer ‚Üí Action Prediction

Model sees:
- Visual observation (where am I?)
- Current state (what am I doing?)
- Predicts next action (what should I do?)
```

### **Simple ACT (Image-Only)**
```
Input Pipeline:
1. Camera Frame (640√ó480) ‚Üí Image Tensor [3, 480, 640]
2. CNN Encoder ‚Üí Visual Features
3. Transformer ‚Üí Action Prediction

Model sees:
- Visual observation (where am I?)
- Predicts action (what should I do?)
```

---

## üìÅ Dataset State Handling

### **Local Dataset Loader** (`local_dataset_loader.py`)

The dataset **ALWAYS includes state**, even if the model doesn't use it:

```python
# In __getitem__ method:
action = torch.tensor([
    sample['steering'],
    sample['throttle']
], dtype=torch.float32)

# Create observation dict matching LeRobot format
observation = {
    'image_front': image,
    'state': action,  # Current state (for state-action pairs)
}

return {
    'observation': observation,
    'action': action,
    # ...
}
```

**Key Point:** The dataset provides `observation.state`, but:
- ‚úÖ Official ACT **uses it** as additional input
- ‚ùå Simple ACT **ignores it** and only uses the image

---

## ü§ñ Inference Behavior

### **Official ACT Inference** (`test_official_act.py`)

```python
def predict_action(policy, preprocessor, postprocessor, 
                   image_tensor, current_state=None, device='cuda'):
    
    # Default state if none provided (neutral steering/throttle)
    if current_state is None:
        current_state = torch.tensor([0.0, 0.0], dtype=torch.float32)
    
    # Prepare batch for ACT
    batch = {
        'observation.images.front': image_tensor.unsqueeze(0).to(device),
        'observation.state': current_state.unsqueeze(0).to(device),  # ‚úÖ Passes state
    }
    
    # Run inference
    action_output = policy.select_action(batch)
```

**‚úÖ During inference, you MUST provide:**
1. Camera image
2. Current state [steering, throttle]

**Default behavior:** If no state provided, uses `[0.0, 0.0]` (neutral)

### **Simple ACT Inference** (`test_inference.py`)

```python
# Model expects only images
predicted_actions = model(images_batch)  # No state needed
```

**‚úÖ During inference, you only need:**
1. Camera image

---

## üéØ Practical Implications

### **For Training Data Collection:**

Both approaches need the same data format:
```
episode_XXXXXX/
‚îú‚îÄ‚îÄ frame_000000.jpg              # 640√ó480 image
‚îú‚îÄ‚îÄ frame_000001.jpg
‚îî‚îÄ‚îÄ episode_data.json             # Contains control samples
    {
        "control_samples": [
            {
                "steering_normalized": 0.5,
                "throttle_normalized": 0.3,
                "system_timestamp": 1234567890.123
            },
            ...
        ],
        "frame_samples": [...]
    }
```

**‚úÖ No change needed in data collection!**

---

### **For Real-Time Inference:**

#### **Official ACT (State-Aware):**
```python
# You need to track and provide current state
current_steering = 0.0
current_throttle = 0.0

while True:
    frame = camera.capture()
    
    # Create state tensor from current values
    current_state = torch.tensor([current_steering, current_throttle])
    
    # Predict next action
    result = policy.predict(frame, current_state)
    
    # Update current state for next iteration
    current_steering = result['steering']
    current_throttle = result['throttle']
    
    # Send to motors
    send_to_motors(current_steering, current_throttle)
```

#### **Simple ACT (Image-Only):**
```python
# Simpler - only needs camera frames
while True:
    frame = camera.capture()
    
    # Predict action directly from image
    result = model(frame)
    
    # Send to motors
    send_to_motors(result[0], result[1])
```

---

## üî¨ Theoretical Differences

### **State-Aware (Official ACT)**

**Advantages:**
- ‚úÖ Model knows current vehicle dynamics
- ‚úÖ Can learn smoother control (knows momentum)
- ‚úÖ Better temporal consistency
- ‚úÖ Can compensate for lag/delays

**Disadvantages:**
- ‚ùå More complex input pipeline
- ‚ùå Requires state tracking during inference
- ‚ùå Initialization issue (what's the first state?)

### **Image-Only (Simple ACT)**

**Advantages:**
- ‚úÖ Simpler architecture
- ‚úÖ Easier deployment (no state tracking)
- ‚úÖ More robust to initialization
- ‚úÖ Purely visual reasoning

**Disadvantages:**
- ‚ùå Cannot directly access current vehicle state
- ‚ùå Must infer dynamics from visual motion
- ‚ùå Potentially less smooth control

---

## üìã Quick Reference Table

| Implementation | Uses State Input? | Input Dimensions | Model Type |
|---------------|------------------|------------------|------------|
| `official_act_trainer.py` | ‚úÖ YES | Image: [3,480,640]<br>State: [2] | Full ACT |
| `train_local_act.py` | ‚úÖ YES | Image: [3,480,640]<br>State: [2] | Full ACT |
| `simple_act_trainer.py` | ‚ùå NO | Image: [3,480,640] | Image-only |
| `enhanced_act_trainer.py` | ‚ùå NO | Image: [3,480,640] | Image-only |

---

## üîß Recommendation for Deployment

### **For Raspberry Pi 5 Deployment:**

1. **If using Official ACT model:**
   - Initialize state to `[0.0, 0.0]` at startup
   - Track and update state at each inference step
   - Pass both image and current state to model

2. **If using Simple ACT model:**
   - Only need camera feed
   - Direct image ‚Üí action prediction
   - Simpler implementation

### **Which to Choose?**

**Use State-Aware (Official ACT) if:**
- You want smoother, more controlled behavior
- You can track vehicle state reliably
- Training data shows good state information

**Use Image-Only (Simple ACT) if:**
- You want simplicity and reliability
- State tracking adds complexity you don't need
- Visual information alone is sufficient

---

## üö® Important Notes

1. **Dataset provides state regardless** - both implementations can use the same training data
2. **Inference differs** - state-aware needs current state input, image-only doesn't
3. **Default values** - if state is missing, official ACT uses `[0.0, 0.0]` (neutral)
4. **State meaning** - `[steering, throttle]` both normalized to [-1, 1] or [0, 1] range

---

## ‚úÖ Final Answer

**YES**, the **Official ACT implementation** (`official_act_trainer.py` and `train_local_act.py`) **DOES pass current states** (steering and throttle) to the model as a 2D state vector `[steering, throttle]` in addition to the camera observations.

**NO**, the **Simple ACT implementation** (`simple_act_trainer.py` and `enhanced_act_trainer.py`) **DOES NOT use state** - it's a purely vision-based model that only takes camera images as input.

**For SmolVLA integration**, you'll likely follow the state-aware pattern since VLA models typically use both visual and state information for grounded action prediction.
